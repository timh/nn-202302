{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ngram_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "numchar=5\n",
      "numhidden=50\n",
      "numdims=20\n",
      "dictsize=27\n",
      "batch_size=2048\n",
      "\n",
      "1/6000: train loss 2.7955, val loss 2.7640 at lr=2.0000\n",
      "2/6000: train loss 2.6438, val loss 2.7142 at lr=2.0000\n",
      "3/6000: train loss 2.6074, val loss 2.6922 at lr=2.0000\n",
      "4/6000: train loss 2.5874, val loss 2.6798 at lr=2.0000\n",
      "5/6000: train loss 2.5750, val loss 2.6719 at lr=2.0000\n",
      "6/6000: train loss 2.5662, val loss 2.6664 at lr=2.0000\n",
      "7/6000: train loss 2.5595, val loss 2.6623 at lr=2.0000\n",
      "8/6000: train loss 2.5542, val loss 2.6591 at lr=2.0000\n",
      "9/6000: train loss 2.5499, val loss 2.6565 at lr=2.0000\n",
      "10/6000: train loss 2.5462, val loss 2.6543 at lr=2.0000\n",
      "11/6000: train loss 2.5431, val loss 2.6524 at lr=2.0000\n",
      "12/6000: train loss 2.5404, val loss 2.6509 at lr=2.0000\n",
      "13/6000: train loss 2.5379, val loss 2.6496 at lr=2.0000\n",
      "14/6000: train loss 2.5358, val loss 2.6486 at lr=2.0000\n",
      "15/6000: train loss 2.5338, val loss 2.6477 at lr=2.0000\n",
      "16/6000: train loss 2.5321, val loss 2.6470 at lr=2.0000\n",
      "17/6000: train loss 2.5305, val loss 2.6463 at lr=2.0000\n",
      "18/6000: train loss 2.5291, val loss 2.6457 at lr=2.0000\n",
      "19/6000: train loss 2.5278, val loss 2.6452 at lr=2.0000\n",
      "20/6000: train loss 2.5267, val loss 2.6447 at lr=2.0000\n",
      "21/6000: train loss 2.5256, val loss 2.6443 at lr=2.0000\n",
      "22/6000: train loss 2.5245, val loss 2.6439 at lr=2.0000\n",
      "23/6000: train loss 2.5236, val loss 2.6435 at lr=2.0000\n",
      "24/6000: train loss 2.5226, val loss 2.6431 at lr=2.0000\n",
      "25/6000: train loss 2.5218, val loss 2.6428 at lr=2.0000\n",
      "26/6000: train loss 2.5209, val loss 2.6425 at lr=2.0000\n",
      "27/6000: train loss 2.5201, val loss 2.6422 at lr=2.0000\n",
      "28/6000: train loss 2.5194, val loss 2.6419 at lr=2.0000\n",
      "29/6000: train loss 2.5187, val loss 2.6417 at lr=2.0000\n",
      "30/6000: train loss 2.5180, val loss 2.6415 at lr=2.0000\n",
      "31/6000: train loss 2.5174, val loss 2.6413 at lr=2.0000\n",
      "32/6000: train loss 2.5168, val loss 2.6411 at lr=2.0000\n",
      "33/6000: train loss 2.5163, val loss 2.6410 at lr=2.0000\n",
      "34/6000: train loss 2.5157, val loss 2.6408 at lr=2.0000\n",
      "35/6000: train loss 2.5152, val loss 2.6407 at lr=2.0000\n",
      "36/6000: train loss 2.5147, val loss 2.6406 at lr=2.0000\n",
      "37/6000: train loss 2.5143, val loss 2.6405 at lr=2.0000\n",
      "38/6000: train loss 2.5138, val loss 2.6403 at lr=2.0000\n",
      "39/6000: train loss 2.5134, val loss 2.6402 at lr=2.0000\n",
      "40/6000: train loss 2.5130, val loss 2.6401 at lr=2.0000\n",
      "41/6000: train loss 2.5125, val loss 2.6400 at lr=2.0000\n",
      "42/6000: train loss 2.5122, val loss 2.6399 at lr=2.0000\n",
      "43/6000: train loss 2.5118, val loss 2.6399 at lr=2.0000\n",
      "44/6000: train loss 2.5114, val loss 2.6398 at lr=2.0000\n",
      "45/6000: train loss 2.5111, val loss 2.6397 at lr=2.0000\n",
      "46/6000: train loss 2.5107, val loss 2.6396 at lr=2.0000\n",
      "47/6000: train loss 2.5104, val loss 2.6396 at lr=2.0000\n",
      "48/6000: train loss 2.5101, val loss 2.6395 at lr=2.0000\n",
      "49/6000: train loss 2.5098, val loss 2.6395 at lr=2.0000\n",
      "50/6000: train loss 2.5095, val loss 2.6394 at lr=2.0000\n",
      "51/6000: train loss 2.5092, val loss 2.6394 at lr=2.0000\n",
      "52/6000: train loss 2.5089, val loss 2.6393 at lr=2.0000\n",
      "53/6000: train loss 2.5086, val loss 2.6393 at lr=2.0000\n",
      "54/6000: train loss 2.5083, val loss 2.6393 at lr=2.0000\n",
      "55/6000: train loss 2.5081, val loss 2.6393 at lr=2.0000\n",
      "56/6000: train loss 2.5078, val loss 2.6392 at lr=2.0000\n",
      "57/6000: train loss 2.5076, val loss 2.6392 at lr=2.0000\n",
      "58/6000: train loss 2.5074, val loss 2.6392 at lr=2.0000\n",
      "59/6000: train loss 2.5071, val loss 2.6392 at lr=2.0000\n",
      "60/6000: train loss 2.5069, val loss 2.6392 at lr=2.0000\n",
      "61/6000: train loss 2.5067, val loss 2.6392 at lr=2.0000\n",
      "62/6000: train loss 2.5064, val loss 2.6392 at lr=2.0000\n",
      "63/6000: train loss 2.5062, val loss 2.6392 at lr=2.0000\n",
      "64/6000: train loss 2.5060, val loss 2.6392 at lr=2.0000\n",
      "65/6000: train loss 2.5058, val loss 2.6392 at lr=2.0000\n",
      "66/6000: train loss 2.5056, val loss 2.6392 at lr=2.0000\n",
      "67/6000: train loss 2.5054, val loss 2.6392 at lr=2.0000\n",
      "68/6000: train loss 2.5052, val loss 2.6392 at lr=2.0000\n",
      "69/6000: train loss 2.5050, val loss 2.6392 at lr=2.0000\n",
      "70/6000: train loss 2.5048, val loss 2.6392 at lr=2.0000\n",
      "71/6000: train loss 2.5046, val loss 2.6392 at lr=2.0000\n",
      "72/6000: train loss 2.5044, val loss 2.6392 at lr=2.0000\n",
      "73/6000: train loss 2.5042, val loss 2.6392 at lr=2.0000\n",
      "74/6000: train loss 2.5041, val loss 2.6392 at lr=2.0000\n",
      "75/6000: train loss 2.5039, val loss 2.6392 at lr=2.0000\n",
      "76/6000: train loss 2.5037, val loss 2.6392 at lr=2.0000\n",
      "77/6000: train loss 2.5035, val loss 2.6392 at lr=2.0000\n",
      "78/6000: train loss 2.5033, val loss 2.6392 at lr=2.0000\n",
      "79/6000: train loss 2.5032, val loss 2.6393 at lr=2.0000\n",
      "80/6000: train loss 2.5030, val loss 2.6393 at lr=2.0000\n",
      "81/6000: train loss 2.5029, val loss 2.6393 at lr=2.0000\n",
      "82/6000: train loss 2.5027, val loss 2.6393 at lr=2.0000\n",
      "83/6000: train loss 2.5026, val loss 2.6394 at lr=2.0000\n",
      "84/6000: train loss 2.5024, val loss 2.6394 at lr=2.0000\n",
      "85/6000: train loss 2.5023, val loss 2.6394 at lr=2.0000\n",
      "86/6000: train loss 2.5022, val loss 2.6395 at lr=2.0000\n",
      "87/6000: train loss 2.5020, val loss 2.6395 at lr=2.0000\n",
      "88/6000: train loss 2.5019, val loss 2.6395 at lr=2.0000\n",
      "89/6000: train loss 2.5017, val loss 2.6395 at lr=2.0000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(ngram_simple)\n",
    "ngram_simple.main(6000, lambda epoch: 1.0 if epoch < 300 else 0.5, numchar=5, numhidden=50, numdims=20, batch_size=2048, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2e4ad7cba72cac7676a24c47a5621d2e895387d4b9b749bc7ccc706239c0d64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
