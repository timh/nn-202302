{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "HBWubMYP2XFS"
      },
      "outputs": [],
      "source": [
        "compounds_str = \"\"\"\n",
        "part1-CO2_50-FeO_50:\n",
        "CO2: 50%\n",
        "FeO: 50%\n",
        "\n",
        "part2-CO_20-Fe_50-AgO_20-AlO_10:\n",
        "CO: 20%\n",
        "Fe: 50%\n",
        "AgO: 20%\n",
        "AlO: 10%\n",
        "\n",
        "part3-HCO2_15-FeO2_60-Ag_25:\n",
        "HCO2: 15%\n",
        "FeO2: 60%\n",
        "Ag: 25%\n",
        "\n",
        "part4-HCO2_20-FeO2_60-Ag_20:\n",
        "HCO2: 20%\n",
        "FeO2: 60%\n",
        "Ag: 20%\n",
        "\"\"\"\n",
        "import torch\n",
        "import datetime\n",
        "from typing import List, Tuple, Literal\n",
        "from dataclasses import dataclass\n",
        "import importlib\n",
        "\n",
        "import mark_essential_oils\n",
        "importlib.reload(mark_essential_oils)\n",
        "from mark_essential_oils import Config, gen_examples, train\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm\n",
        "from matplotlib.ticker import LogLocator\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from IPython import display\n",
        "\n",
        "# import seaborn as sns\n",
        "# import pandas\n",
        "# sns.set_theme()\n",
        "\n",
        "@dataclass\n",
        "class Annotation:\n",
        "    text: str\n",
        "    xy: Tuple[float, float]\n",
        "    axes: str\n",
        "    textxy: Tuple[float, float] = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "num_hidden 4, hidden_size 36\n",
            "lr = 3e-05 (num_hidden 4, hidden_size 36)\n",
            "epoch 162/5000: train loss 0.85961, val loss 0.86046, val dist 0.04790\n",
            "epoch 316/5000: train loss 0.82853, val loss 0.83241, val dist 0.03074\n",
            "epoch 475/5000: train loss 0.81487, val loss 0.81834, val dist 0.02667\n",
            "epoch 633/5000: train loss 0.81387, val loss 0.81870, val dist 0.02464\n",
            "epoch 789/5000: train loss 0.81102, val loss 0.81458, val dist 0.02258\n",
            "epoch 949/5000: train loss 0.80364, val loss 0.80903, val dist 0.02047\n",
            "epoch 1112/5000: train loss 0.80055, val loss 0.80361, val dist 0.01803\n",
            "epoch 1276/5000: train loss 0.79656, val loss 0.80172, val dist 0.01696\n",
            "epoch 1436/5000: train loss 0.79530, val loss 0.79844, val dist 0.01619\n",
            "epoch 1594/5000: train loss 0.79399, val loss 0.79927, val dist 0.01584\n",
            "epoch 1756/5000: train loss 0.79886, val loss 0.80045, val dist 0.01564\n",
            "epoch 1916/5000: train loss 0.79513, val loss 0.79692, val dist 0.01499\n",
            "epoch 2079/5000: train loss 0.78969, val loss 0.79380, val dist 0.01391\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%matplotlib inline\n",
        "cfg = Config(compounds_str)\n",
        "cfg.setup(num_hidden=3, hidden_size=len(cfg.all_mol_names), lr=1e-4)\n",
        "\n",
        "annotations: List[Annotation] = list()\n",
        "\n",
        "color_val = \"blue\"\n",
        "color_lr = \"green\"\n",
        "fig = plt.gcf()\n",
        "\n",
        "def reset_plot(val_loss_min: float, val_loss_max: float, val_dist_min: float, val_dist_max: float):\n",
        "    plt.rcParams[\"figure.figsize\"] = (30, 20) # (w, h)\n",
        "    fig.clear()\n",
        "    ax_val_losses = fig.add_subplot(2, 1, 1, label=\"val loss\")\n",
        "    ax_val_dists = fig.add_subplot(2, 1, 2, label=\"val distance\")\n",
        "\n",
        "    all_axes = [ax_val_losses, ax_val_dists]\n",
        "    all_axes_lr = [ax.twinx() for ax in all_axes]\n",
        "\n",
        "    for ax in all_axes_lr:\n",
        "        ax.set_yscale(\"log\")\n",
        "        ax.set_label(\"learning rate\")\n",
        "        ax.set_ylabel(ax.get_label(), color=color_lr)\n",
        "        ax.yaxis.set_major_locator(LogLocator(subs='all'))\n",
        "        ax.yaxis.set_minor_locator(LogLocator(subs='all'))\n",
        "\n",
        "    for ax in all_axes:\n",
        "        ax.set_yscale(\"log\")\n",
        "        ax.set_ylabel(ax.get_label(), color=color_val)\n",
        "        ax.yaxis.set_major_locator(LogLocator(subs='all'))\n",
        "        ax.yaxis.set_minor_locator(LogLocator(subs='all'))\n",
        "\n",
        "    ax_val_losses.set_ylim(bottom=val_loss_min, top=val_loss_max + 0.1)\n",
        "    ax_val_dists.set_ylim(bottom=val_dist_min, top=val_dist_max + 0.1)\n",
        "\n",
        "    return all_axes, all_axes_lr\n",
        "\n",
        "num_mol = len(cfg.all_mol_names)\n",
        "hp_num_batches = 10\n",
        "hp_batch_size = 1000\n",
        "hp_learning_rates = {3e-5: 5000, 1e-5: 5000, 3e-6: 5000, 2e-6: 5000, 1e-6: 5000}\n",
        "hp_num_hidden = [4, 6, 8, 10]\n",
        "# hp_hidden_size = [num_mol * 6, num_mol * 8, num_mol * 10]\n",
        "# hp_hidden_size = [num_mol * 6]\n",
        "hp_hidden_size = [i * num_mol for i in range(4, 12, 2)]\n",
        "\n",
        "# TODO - only for fast debugging\n",
        "# hp_learning_rates = {k: v//100 for k, v in hp_learning_rates.items()}\n",
        "\n",
        "data_train = gen_examples(cfg, hp_num_batches, hp_batch_size)\n",
        "data_val = gen_examples(cfg, hp_num_batches, hp_batch_size)\n",
        "\n",
        "train_loss_hist = torch.zeros((sum(hp_learning_rates.values()) * len(hp_num_hidden) * len(hp_hidden_size), ))\n",
        "val_loss_hist = torch.zeros_like(train_loss_hist)\n",
        "val_loss_min, val_loss_max = 1000000.0, 0.0\n",
        "val_dist_hist = torch.zeros_like(train_loss_hist)\n",
        "val_dist_min, val_dist_max = 1000000.0, 0.0\n",
        "lr_hist = torch.ones_like(train_loss_hist)\n",
        "\n",
        "timestr = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "base_filename = [\"outputs\", \"mark_essential_oils2\", \"results-\" + timestr]\n",
        "base_filename = \"/\".join(base_filename)\n",
        "\n",
        "config_idx = 0\n",
        "total_epochs = 0\n",
        "for num_hidden in hp_num_hidden:\n",
        "    for hidden_size in hp_hidden_size:\n",
        "        cfg.setup(num_hidden=num_hidden, hidden_size=hidden_size, lr=1e-4)\n",
        "\n",
        "        print()\n",
        "        print(f\"num_hidden {num_hidden}, hidden_size {hidden_size}\")\n",
        "        epochs_at_lr = 0\n",
        "\n",
        "        for lridx, (lr, epochs) in enumerate(hp_learning_rates.items()):\n",
        "            print(f\"lr = {lr} (num_hidden {num_hidden}, hidden_size {hidden_size})\")\n",
        "            cfg.optim = torch.optim.AdamW(cfg.net.parameters(), lr=lr)\n",
        "\n",
        "            tlosses, vlosses, vdists = train(cfg, epochs, data_train, data_val)\n",
        "\n",
        "            size_sofar = total_epochs + epochs\n",
        "            idx_sofar = size_sofar - 1\n",
        "\n",
        "            train_loss_hist[total_epochs:total_epochs + epochs] = tlosses\n",
        "            val_loss_hist[total_epochs:total_epochs + epochs] = vlosses\n",
        "            val_dist_hist[total_epochs:total_epochs + epochs] = vdists\n",
        "            lr_hist[total_epochs:total_epochs + epochs] *= lr\n",
        "\n",
        "            # throw away the top of loss & distance, as they are probably the first epochs of \n",
        "            # the run and the network is totally untrained.\n",
        "            val_loss_max = max(val_loss_max, torch.quantile(vlosses[:size_sofar], torch.tensor(0.9)).item())\n",
        "            val_dist_max = max(val_dist_max, torch.quantile(vdists[:size_sofar], torch.tensor(0.9)).item())\n",
        "            val_loss_min = min(val_loss_min, torch.min(vlosses[:size_sofar]).item())\n",
        "            val_dist_min = min(val_dist_min, torch.min(vdists[:size_sofar]).item())\n",
        "\n",
        "            all_axes, all_axes_lr = reset_plot(val_loss_min, val_loss_max, val_dist_min, val_dist_max)\n",
        "            all_vals_sofar = [val_loss_hist[:size_sofar], val_dist_hist[:size_sofar]]\n",
        "            lr_hist_sofar = lr_hist[:size_sofar]\n",
        "\n",
        "            # annotation for hyper param change\n",
        "            lrpos = (float(total_epochs), lr_hist[total_epochs].item())\n",
        "            if lridx == 0:\n",
        "                annotext = f\"n/hid {num_hidden}\\nhid sz {hidden_size}\"\n",
        "                anno = Annotation(text=annotext, xy=lrpos, axes=\"learning rate\")\n",
        "                annotations.append(anno)\n",
        "\n",
        "            # annotation for ending loss for that learning rate\n",
        "            textpos = (idx_sofar, lr_hist_sofar[-1].item())\n",
        "            annotext = f\"loss {val_loss_hist[idx_sofar]:.4f}\"\n",
        "            annopos = (textpos[0], val_loss_hist[idx_sofar].item())\n",
        "            anno = Annotation(annotext, xy=annopos, axes=\"val loss\") #, textxy=textpos)\n",
        "            annotations.append(anno)\n",
        "\n",
        "            annotext = f\"dist {val_dist_hist[idx_sofar]:.4f}\"\n",
        "            annopos = (textpos[0], val_dist_hist[idx_sofar])\n",
        "            anno = Annotation(annotext, xy=annopos, axes=\"val distance\") #, textxy=textpos)\n",
        "            annotations.append(anno)\n",
        "\n",
        "            for axes, axes_lr, vals in zip(all_axes, all_axes_lr, all_vals_sofar):\n",
        "                axes.plot(vals, color=color_val, label=axes.get_label())\n",
        "                lines, labels = axes.get_legend_handles_labels()\n",
        "\n",
        "                axes_lr.plot(lr_hist_sofar[:-1], color=color_lr, label=axes_lr.get_label())\n",
        "                lines_lr, labels_lr = axes_lr.get_legend_handles_labels()\n",
        "\n",
        "                axes.legend(lines + lines_lr, labels + labels_lr)\n",
        "\n",
        "                for anno in annotations:\n",
        "                    kvargs = {}\n",
        "                    use_axes: plt.Axes = None\n",
        "                    if anno.axes == axes_lr.get_label():\n",
        "                        xoff = 2\n",
        "                        yoff = (anno.text.count(\"\\n\") + 1) * -15\n",
        "                        color = color_lr\n",
        "                        use_axes = axes_lr\n",
        "\n",
        "                    elif anno.axes == axes.get_label():\n",
        "                        xoff, yoff = -30, 50\n",
        "                        color = color_val\n",
        "                        use_axes = axes\n",
        "\n",
        "                        if anno.textxy is not None:\n",
        "                            # con = ConnectionPatch(xyA=anno.xy, axesA=axes,\n",
        "                            #                       xyB=anno.textxy, axesB=axes_lr,\n",
        "                            #                       coordsA=\"data\", coordsB=\"data\")\n",
        "                            # axes_lr.add_artist(con)\n",
        "                            kvargs = dict(xytext=anno.textxy, textcoords=axes_lr.transData, arrowprops=dict(arrowstyle='->'))\n",
        "                        else:\n",
        "                            kvargs = dict(xytext=(xoff, yoff), textcoords='offset pixels', arrowprops=dict(arrowstyle='->'))\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    if not kvargs:\n",
        "                        kvargs = dict(xytext=(xoff, yoff), textcoords='offset pixels')\n",
        "                    use_axes.annotate(text=anno.text, xy=anno.xy, color=color, **kvargs)\n",
        "                \n",
        "            display.clear_output(True)\n",
        "            display.display(fig)\n",
        "\n",
        "            epochs_at_lr += epochs\n",
        "            total_epochs += epochs\n",
        "\n",
        "            torch_filename = f\"{base_filename}-lr_{lr}-num_hidden_{num_hidden}-hidden_size_{hidden_size}-epoch{epochs_at_lr:05}.torch\"\n",
        "            torch.save(cfg.net, torch_filename)\n",
        "            img_filename = f\"{base_filename}.png\"\n",
        "            fig.savefig(img_filename)\n",
        "            print(f\"saved {img_filename} & {torch_filename}\")\n",
        "\n",
        "            for ax in [*all_axes, *all_axes_lr]:\n",
        "                ax.clear()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensors = [m.weight.grad for m in cfg.net.modules() if isinstance(m, torch.nn.Linear)]\n",
        "# tensors = [t.abs() > 0.001 for t in tensors]\n",
        "tensors = [t.abs() < .01 for t in tensors]\n",
        "# tensors = [torch.clamp(t, min=-1.0+1e-7, max=1.0-1e-7) for t in tensors]\n",
        "# tensors = [m.weight for m in cfg.net.modules() if isinstance(m, torch.nn.Linear)]\n",
        "count_all_rows = max([t.shape[0] for t in tensors])\n",
        "count_all_cols = sum([t.shape[1] for t in tensors])\n",
        "\n",
        "all_tensors = torch.zeros((count_all_rows, count_all_cols))\n",
        "running_col = 0\n",
        "for t in tensors:\n",
        "    trows, tcols = t.shape\n",
        "    print(f\"trows {trows}, tcols {tcols}\")\n",
        "    print(f\"[:{trows}, {running_col}:{running_col+tcols}] = {t.shape}\")\n",
        "    all_tensors[:trows, running_col:running_col+tcols] = t\n",
        "    running_col += tcols\n",
        "\n",
        "plt.matshow(all_tensors.detach().cpu(), cmap=matplotlib.cm.gray, vmin=-1, vmax=1)\n",
        "all_tensors\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "diffusers",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "f2e4ad7cba72cac7676a24c47a5621d2e895387d4b9b749bc7ccc706239c0d64"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
